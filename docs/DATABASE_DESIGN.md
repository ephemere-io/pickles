# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆï¼ˆPhase 0: å®Ÿè¡Œå±¥æ­´ç®¡ç†ï¼‰

## ğŸ“‹ æ¦‚è¦

Phase 0ã§ã¯ã€**Google Sheetsã‚’ç¶™ç¶šä½¿ç”¨ã—ã¤ã¤ã€Supabaseã§å®Ÿè¡Œå±¥æ­´ã‚’è¨˜éŒ²**ã—ã¾ã™ã€‚
Google Sheetsã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ãƒã‚¹ã‚¿ãƒ¼ï¼ˆçœŸå®Ÿã®æºæ³‰ï¼‰ã¨ã—ã¦ç¶­æŒã—ã€Supabaseã¯å®Ÿè¡Œå±¥æ­´ã®æ°¸ç¶šåŒ–ã¨UUIDç®¡ç†ã«ç‰¹åŒ–ã—ã¾ã™ã€‚

## ğŸ¯ å‰æã¨ã™ã‚‹é‹ç”¨å½¢æ…‹

### Google Sheetsã‚’ç¶™ç¶šä½¿ç”¨

æœ¬è¨­è¨ˆã§ã¯ã€**ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ç®¡ç†ã¯Google Sheetsã§ç¶™ç¶š**ã—ã¾ã™ã€‚

**Google Sheetsã®æ§‹é€ **:
```
åˆ—A: EMAIL_TO         ä¾‹: dominick.chen@gmail.com
åˆ—B: NOTION_API_KEY   ä¾‹: ntn_G17609648604...
åˆ—C: GOOGLE_DOCS_URL  ä¾‹: (ç©ºæ¬„ã¾ãŸã¯URL)
åˆ—D: USER_NAME        ä¾‹: Dominique
åˆ—E: LANGUAGE         ä¾‹: japanese
```

**é‹ç”¨ãƒ•ãƒ­ãƒ¼**:
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½åŠ : Google Sheetsã«è¡Œã‚’è¿½åŠ 
2. ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å¤‰æ›´: Google Sheetsä¸Šã§ç·¨é›†ï¼ˆAPI Keyå¤‰æ›´ãªã©ï¼‰
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰Šé™¤: Google Sheetsã‹ã‚‰è¡Œã‚’å‰Šé™¤
4. æ¬¡å›å®Ÿè¡Œæ™‚: è‡ªå‹•çš„ã«Supabaseã¨åŒæœŸ

### Supabaseã®å½¹å‰²

Supabaseã¯**å®Ÿè¡Œå±¥æ­´ã®è¨˜éŒ²**ã¨**UUIDç®¡ç†**ã«ç‰¹åŒ–:

**usersãƒ†ãƒ¼ãƒ–ãƒ«**:
- Google Sheetsã‹ã‚‰åŒæœŸã•ã‚ŒãŸæƒ…å ±ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ â†’ UUID ã®å¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«
- æœ€çµ‚åˆ†ææ™‚åˆ»ã®è¨˜éŒ²

**analysis_runs / deliveriesãƒ†ãƒ¼ãƒ–ãƒ«**:
- åˆ†æå®Ÿè¡Œå±¥æ­´ã®æ°¸ç¶šåŒ–
- ã‚¨ãƒ©ãƒ¼è¿½è·¡ã€é…ä¿¡å±¥æ­´ã®ç®¡ç†

**é‡è¦**: usersãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ç›´æ¥CRUDæ“ä½œã¯åŸå‰‡ã¨ã—ã¦è¡Œã„ã¾ã›ã‚“ã€‚Google SheetsãŒçœŸå®Ÿã®æºæ³‰ã§ã™ã€‚

---

## ğŸ¯ è¨­è¨ˆåŸå‰‡

1. **Google Sheetsã‚’çœŸå®Ÿã®æºæ³‰ã¨ã—ã¦ç¶­æŒ**
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ç·¨é›†ã¯Google Sheetsã§å®Ÿæ–½
   - Supabaseã¯è‡ªå‹•åŒæœŸã•ã‚Œã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥

2. **JSONBä½¿ç”¨ã‚¼ãƒ­**
   - ã™ã¹ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªTEXTå‹ã§ç®¡ç†
   - è¤‡é›‘æ€§ã‚’æ’é™¤ã—ã€ä¿å®ˆæ€§ã‚’å‘ä¸Š

3. **é–¢å¿ƒã®åˆ†é›¢**
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†ï¼ˆ`users`ï¼‰â† Google Sheetsã‹ã‚‰åŒæœŸ
   - åˆ†æå®Ÿè¡Œã¨çµæœï¼ˆ`analysis_runs`ï¼‰
   - é…ä¿¡æ–¹æ³•ï¼ˆ`deliveries`ï¼‰

4. **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ éä¾å­˜**
   - GitHub Actionså°‚ç”¨ã®è¨­è¨ˆã‚’æ’é™¤
   - Cronã€æ‰‹å‹•å®Ÿè¡Œã€APIçµŒç”±ãªã©å°†æ¥ã®æ‹¡å¼µã«å¯¾å¿œ

5. **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨è¡¨ç¾ã®åˆ†é›¢**
   - é…ä¿¡ç”¨ãƒ¬ãƒãƒ¼ãƒˆã¯`analysis_runs.content`ã«1å›ã ã‘ä¿å­˜
   - `deliveries`ã¯é…ä¿¡æ–¹æ³•ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿æŒ

---

## ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«è¨­è¨ˆ

### 1. `users` ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†ãƒ»UUIDå¤‰æ›ï¼‰

```sql
create table public.users (
    id uuid primary key default gen_random_uuid(),
    email text unique not null,  -- Google Sheetsã¨ã®ç…§åˆã‚­ãƒ¼
    user_name text not null,

    -- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ï¼ˆGoogle Sheetsã‹ã‚‰åŒæœŸï¼‰
    notion_api_key text,
    google_docs_url text,
    source_type text check (source_type in ('notion', 'gdocs', 'both')),

    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    language text default 'japanese',
    is_active boolean default true,  -- Sheetsã‹ã‚‰å‰Šé™¤æ™‚false

    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at timestamptz default now(),    -- UUIDä»˜ä¸æ—¥æ™‚
    updated_at timestamptz default now(),    -- æœ€çµ‚åŒæœŸæ—¥æ™‚
    last_analysis_at timestamptz             -- æœ€çµ‚åˆ†æå®Ÿè¡Œæ—¥æ™‚
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
create index idx_users_email on public.users(email);
create index idx_users_active on public.users(is_active) where is_active = true;
create index idx_users_last_analysis_at on public.users(last_analysis_at);
```

**ç›®çš„**: Google Sheetsã‹ã‚‰åŒæœŸã•ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ + UUIDç®¡ç†

**ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å½¹å‰²**:

| ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ | å½¹å‰² | æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚° |
|-----------|------|--------------|
| `id` | UUIDï¼ˆæ°¸ç¶šçš„ãªè­˜åˆ¥å­ï¼‰ | åˆå›ç™»éŒ²æ™‚ã«è‡ªå‹•ç”Ÿæˆ |
| `email` | Google Sheetsã¨ã®ç…§åˆã‚­ãƒ¼ | - |
| `user_name` | ãƒ¦ãƒ¼ã‚¶ãƒ¼å | æ¯å›åŒæœŸæ™‚ã«æ›´æ–° |
| `notion_api_key` | Notion API Key | æ¯å›åŒæœŸæ™‚ã«æ›´æ–° |
| `google_docs_url` | Google Docs URL | æ¯å›åŒæœŸæ™‚ã«æ›´æ–° |
| `source_type` | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç¨®åˆ¥ | æ¯å›åŒæœŸæ™‚ã«åˆ¤å®š |
| `language` | è¨€èªè¨­å®š | æ¯å›åŒæœŸæ™‚ã«æ›´æ–° |
| `is_active` | ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ | Sheetsã‹ã‚‰å‰Šé™¤æ™‚false |
| `created_at` | UUIDä»˜ä¸æ—¥æ™‚ | åˆå›ç™»éŒ²æ™‚ã®ã¿ |
| `updated_at` | æœ€çµ‚åŒæœŸæ—¥æ™‚ | æ¯å›åŒæœŸæ™‚ã«æ›´æ–° |
| `last_analysis_at` | æœ€çµ‚åˆ†ææ—¥æ™‚ | main.pyå®Ÿè¡Œå®Œäº†æ™‚ã«æ›´æ–° |

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**:
1. **UUIDã®æ°¸ç¶šæ€§**: ä¸€åº¦ä»˜ä¸ã•ã‚ŒãŸUUIDã¯å¤‰æ›´ã•ã‚Œãªã„
2. **Google SheetsãŒçœŸå®Ÿã®æºæ³‰**: usersãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«éããªã„
3. **å‰Šé™¤ã¯è«–ç†å‰Šé™¤**: `is_active = false`ã«ã—ã¦å±¥æ­´ã‚’ä¿æŒ

---

### 2. `analysis_runs` ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆåˆ†æå®Ÿè¡Œã¨çµæœï¼‰

```sql
create table public.analysis_runs (
    id uuid primary key default gen_random_uuid(),
    user_id uuid not null references public.users(id) on delete cascade,

    -- åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    analysis_type text not null check (analysis_type in ('domi', 'aga')),
    days_analyzed integer not null,
    source_used text not null check (source_used in ('notion', 'gdocs')),

    -- åˆ†æçµæœï¼ˆé…ä¿¡ç”¨ãƒ¬ãƒãƒ¼ãƒˆï¼‰
    content text,          -- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é…ä¿¡ã—ãŸãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡ï¼ˆMarkdownå½¢å¼ï¼‰
    stats_summary text,    -- çµ±è¨ˆã‚µãƒãƒªãƒ¼ï¼ˆä¾‹: "ç›´è¿‘7æ—¥é–“: 21ä»¶ã€å¹³å‡5092æ–‡å­—"ï¼‰

    -- å®Ÿè¡Œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    status text not null check (status in ('pending', 'running', 'completed', 'failed')),
    error_message text,
    trigger_type text check (trigger_type in ('github_actions', 'manual', 'cron', 'api')),
    trigger_id text,

    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at timestamptz default now(),
    completed_at timestamptz
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
create index idx_analysis_runs_user_id on public.analysis_runs(user_id);
create index idx_analysis_runs_status on public.analysis_runs(status);
create index idx_analysis_runs_created_at on public.analysis_runs(created_at desc);
```

**ç›®çš„**:
- **ä½•ã‚’**ï¼ˆanalysis_type, days_analyzedï¼‰
- **ã©ã†**åˆ†æã—ãŸã‹ï¼ˆsource_usedï¼‰
- **çµæœ**ã¯ã©ã†ã ã£ãŸã‹ï¼ˆcontent, stats_summary, statusï¼‰

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
- `content`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é…ä¿¡ã—ãŸãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡ï¼ˆMarkdownå½¢å¼ï¼‰
  - DOMI/AGAåˆ†æçµæœã‚’å«ã‚€å®Œæˆå½¢
  - é…ä¿¡å±¥æ­´ã¨ã—ã¦ä¿æŒï¼ˆã€Œä½•ã‚’é€ã£ãŸã‹ã€ã®è¨˜éŒ²ï¼‰
  - å†é€æ©Ÿèƒ½ã‚„ã‚¨ãƒ©ãƒ¼èª¿æŸ»ã«ä½¿ç”¨
- `trigger_type` + `trigger_id`: å®Ÿè¡Œãƒˆãƒªã‚¬ãƒ¼ã‚’æ§‹é€ åŒ–ã—ã¦è¨˜éŒ²

**`trigger_type` ã¨ `trigger_id` ã®è¨˜éŒ²ä¾‹**:
```python
# GitHub Actionså®Ÿè¡Œæ™‚
trigger_type = 'github_actions'
trigger_id = os.getenv('GITHUB_RUN_ID')  # ä¾‹: '123456'

# æ‰‹å‹•å®Ÿè¡Œæ™‚
trigger_type = 'manual'
trigger_id = 'user@example.com'

# Cronå®Ÿè¡Œæ™‚
trigger_type = 'cron'
trigger_id = 'weekly_monday_7am'

# APIçµŒç”±æ™‚
trigger_type = 'api'
trigger_id = 'webhook_xyz'
```

---

### 3. `deliveries` ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆé…ä¿¡å±¥æ­´ï¼‰

```sql
create table public.deliveries (
    id uuid primary key default gen_random_uuid(),
    analysis_run_id uuid not null references public.analysis_runs(id) on delete cascade,

    -- é…ä¿¡æ–¹æ³•
    delivery_method text not null check (delivery_method in (
        'email_html', 'email_text', 'console', 'file_text', 'file_html'
    )),
    email_to text,  -- é…ä¿¡å…ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆemailé…ä¿¡æ™‚ã®ã¿ï¼‰

    -- é…ä¿¡çµæœ
    status text not null check (status in ('pending', 'sent', 'failed')),
    error_message text,

    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at timestamptz default now(),
    sent_at timestamptz
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
create index idx_deliveries_analysis_run_id on public.deliveries(analysis_run_id);
create index idx_deliveries_status on public.deliveries(status);
create index idx_deliveries_created_at on public.deliveries(created_at desc);
```

**ç›®çš„**:
- **ã©ã®æ–¹æ³•ã§**ï¼ˆdelivery_methodï¼‰
- **èª°ã«**ï¼ˆemail_toï¼‰
- **å±Šã‘ãŸã‹**ï¼ˆstatusï¼‰

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ä¿å­˜ã›ãšã€`analysis_run_id`ã§å‚ç…§ã™ã‚‹ã®ã¿
- 1ã¤ã®åˆ†æçµæœã‚’è¤‡æ•°ã®æ–¹æ³•ã§é…ä¿¡å¯èƒ½ï¼ˆä¾‹: email_html + email_textï¼‰
- JSONBä¸ä½¿ç”¨

---

## ğŸ”„ è‡ªå‹•åŒæœŸã®ä»•çµ„ã¿

### åŒæœŸã‚¿ã‚¤ãƒŸãƒ³ã‚°

`read_spreadsheet_and_execute.py`å®Ÿè¡Œæ™‚ã«æ¯å›è‡ªå‹•åŒæœŸã•ã‚Œã¾ã™:

```
GitHub Actionså®Ÿè¡Œï¼ˆé€±æ¬¡ï¼‰
    â†“
Google Sheetsèª­ã¿è¾¼ã¿
    â†“
Supabaseã¨è‡ªå‹•åŒæœŸï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã§ç…§åˆï¼‰
    â”œâ”€ æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ â†’ UUIDä»˜ä¸ã—ã¦ç™»éŒ²
    â”œâ”€ æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ â†’ æƒ…å ±æ›´æ–°ï¼ˆAPI keyå¤‰æ›´å¯¾å¿œï¼‰
    â””â”€ å‰Šé™¤ãƒ¦ãƒ¼ã‚¶ãƒ¼ â†’ æ¤œå‡ºã—ã¦éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
    â†“
å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦åˆ†æå®Ÿè¡Œï¼ˆuser_idä»˜ãï¼‰
    â†“
analysis_runsä¿å­˜ï¼ˆå®Ÿè¡Œå±¥æ­´ï¼‰
deliveriesä¿å­˜ï¼ˆé…ä¿¡å±¥æ­´ï¼‰
```

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è©³ç´°

```
Google Sheetsï¼ˆçœŸå®Ÿã®æºæ³‰ï¼‰
    â†“ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ã‚­ãƒ¼ã«ç…§åˆ
Supabase usersï¼ˆUUIDç®¡ç† + åŒæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    â”œâ”€ æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ â†’ è‡ªå‹•ä½œæˆã—ã¦UUIDä»˜ä¸
    â””â”€ æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ â†’ UUIDå–å¾—ã€æƒ…å ±æ›´æ–°
    â†“
main.pyå®Ÿè¡Œï¼ˆ--user-idæ¸¡ã™ï¼‰
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analysis_runs ãƒ†ãƒ¼ãƒ–ãƒ«   â”‚
â”‚ - analysis_type: 'domi'â”‚
â”‚ - days_analyzed: 7     â”‚
â”‚ - content: "ä»Šé€±ã®..."  â”‚
â”‚ - stats_summary: "21ä»¶"â”‚
â”‚ - status: 'completed'  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ deliveries ãƒ†ãƒ¼ãƒ–ãƒ«      â”‚
â”‚ - delivery_method:     â”‚
â”‚   'email_html'         â”‚
â”‚ - email_to:            â”‚
â”‚   'user@example.com'   â”‚
â”‚ - status: 'sent'       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ é‹ç”¨ãƒ•ãƒ­ãƒ¼

### ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½åŠ 

```
1. Google Sheetsã«æ–°ã—ã„è¡Œã‚’è¿½åŠ 
   EMAIL_TO | NOTION_API_KEY | ... | USER_NAME | LANGUAGE
   new@example.com | ntn_xxx... | ... | æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ | japanese

2. æ¬¡å›ã®GitHub Actionså®Ÿè¡Œã‚’å¾…ã¤ï¼ˆæ¯é€±æœˆæ›œ7:00ï¼‰
   ã¾ãŸã¯æ‰‹å‹•å®Ÿè¡Œ

3. è‡ªå‹•çš„ã«Supabaseã«UUIDä»˜ãã§ç™»éŒ²ã•ã‚Œã‚‹
   âœ¨ æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™»éŒ²: new@example.com (12345678-...)

4. åˆ†æãŒå®Ÿè¡Œã•ã‚Œã€ãƒ¬ãƒãƒ¼ãƒˆãŒé…ä¿¡ã•ã‚Œã‚‹
```

### ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å¤‰æ›´

```
1. Google Sheetsã§è©²å½“è¡Œã‚’ç·¨é›†
   ä¾‹: Notion API Keyã‚’å¤‰æ›´

2. æ¬¡å›å®Ÿè¡Œæ™‚ã«è‡ªå‹•çš„ã«åæ˜ 
   âœ“ æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼æ›´æ–°: user@example.com
```

### ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰Šé™¤

```
1. Google Sheetsã‹ã‚‰è©²å½“è¡Œã‚’å‰Šé™¤

2. æ¬¡å›å®Ÿè¡Œæ™‚ã«éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
   âš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–: removed@example.com

3. éå»ã®åˆ†æå±¥æ­´ã¯ä¿æŒã•ã‚Œã‚‹ï¼ˆis_active = falseï¼‰

4. å†åº¦è¿½åŠ ã™ã‚Œã°ã€æ—¢å­˜ã®UUIDã§å¾©æ´»
   âœ“ æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼æ›´æ–°: removed@example.com (is_active: true)
```

---

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä¸­å¿ƒã¨ã—ãŸæ§‹é€ ã§ã€å†åˆ©ç”¨æ€§ã¨ä¿å®ˆæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™:

```
pickles/
â”œâ”€â”€ models/                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ user.py               # Userãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ analysis_run.py       # AnalysisRunãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
â”‚   â””â”€â”€ delivery.py           # Deliveryãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
â”‚
â”œâ”€â”€ db/                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£
â”‚   â”œâ”€â”€ migrations/           # ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”‚   â”œâ”€â”€ 20241215000000_create_users_table.sql
â”‚   â”‚   â”œâ”€â”€ 20241215000001_create_analysis_runs_table.sql
â”‚   â”‚   â”œâ”€â”€ 20241215000002_create_deliveries_table.sql
â”‚   â”‚   â””â”€â”€ 20241215000003_create_execution_history_view.sql
â”‚   â””â”€â”€ client.py             # Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
â”‚
â”œâ”€â”€ inputs/                    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
â”‚   â”œâ”€â”€ notion_reader.py      # models.user.get_entries() ã‚’ä½¿ç”¨
â”‚   â””â”€â”€ gdocs_reader.py       # models.user.get_entries() ã‚’ä½¿ç”¨
â”‚
â”œâ”€â”€ throughput/                # åˆ†æãƒ­ã‚¸ãƒƒã‚¯
â”‚   â”œâ”€â”€ domi_analyzer.py      # models.analysis_run ã‚’ä½¿ç”¨
â”‚   â””â”€â”€ aga_analyzer.py       # models.analysis_run ã‚’ä½¿ç”¨
â”‚
â”œâ”€â”€ outputs/                   # é…ä¿¡ãƒ­ã‚¸ãƒƒã‚¯
â”‚   â”œâ”€â”€ email_sender.py       # models.delivery ã‚’ä½¿ç”¨
â”‚   â””â”€â”€ file_writer.py        # models.delivery ã‚’ä½¿ç”¨
â”‚
â”œâ”€â”€ main.py                    # ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆï¼ˆå˜ä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼å®Ÿè¡Œï¼‰
â”œâ”€â”€ read_spreadsheet_and_execute.py  # ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼å®Ÿè¡Œ
â””â”€â”€ utils/
    â”œâ”€â”€ logger.py
    â””â”€â”€ google_service.py
```

**è¨­è¨ˆæ€æƒ³**:
- **ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«**: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã¨æ°¸ç¶šåŒ–ãƒ­ã‚¸ãƒƒã‚¯ã‚’é›†ç´„
- **ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†é›¢**: inputs/throughput/outputsã¯å„ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’importã—ã¦ä½¿ç”¨
- **å†åˆ©ç”¨æ€§**: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¤‡æ•°ç®‡æ‰€ã‹ã‚‰å‘¼ã³å‡ºã—å¯èƒ½
- **ä¿å®ˆæ€§**: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯å¤‰æ›´æ™‚ã¯models/å†…ã‚’ä¿®æ­£ã™ã‚‹ã®ã¿

---

## ğŸ› ï¸ å®Ÿè£…æ‰‹é †

### Step 1: Supabaseãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆï¼ˆ10åˆ†ï¼‰

1. https://supabase.com ã«ã‚¢ã‚¯ã‚»ã‚¹
2. æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
3. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: `pickles-production`
4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰è¨­å®š
5. ãƒªãƒ¼ã‚¸ãƒ§ãƒ³é¸æŠ: `Northeast Asia (Tokyo)`
6. Project URLã¨API Keyã‚’å–å¾—:
   - Settings > API ã‚¿ãƒ–ã‚’é–‹ã
   - **Project URL** â†’ `SUPABASE_URL` ã¨ã—ã¦ä½¿ç”¨
   - **Secret Key** (`sb_secret_...` ã§å§‹ã¾ã‚‹ã‚­ãƒ¼) â†’ `SUPABASE_KEY` ã¨ã—ã¦ä½¿ç”¨

   æ³¨: ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãªã®ã§Secret Keyã‚’ä½¿ç”¨ã—ã¾ã™

---

### Step 2: ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆ10åˆ†ï¼‰

Supabaseã®æœ€å¤§æ´»ç”¨ã®ãŸã‚ã€ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ã—ã¾ã™ã€‚

**ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ**:
```bash
mkdir -p db/migrations
```

**1. `db/migrations/20241215000000_create_users_table.sql`**:
```sql
-- usersãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
create table public.users (
    id uuid primary key default gen_random_uuid(),
    email text unique not null,
    user_name text not null,

    -- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±
    notion_api_key text,
    google_docs_url text,
    source_type text check (source_type in ('notion', 'gdocs', 'both')),

    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    language text default 'japanese',
    is_active boolean default true,

    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at timestamptz default now(),
    updated_at timestamptz default now(),
    last_analysis_at timestamptz
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
create index idx_users_email on public.users(email);
create index idx_users_active on public.users(is_active) where is_active = true;
create index idx_users_last_analysis_at on public.users(last_analysis_at);

-- RLS (Row Level Security) æœ‰åŠ¹åŒ–
alter table public.users enable row level security;

-- ãƒãƒªã‚·ãƒ¼: ã‚µãƒ¼ãƒ“ã‚¹ãƒ­ãƒ¼ãƒ«ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
create policy "Enable all access for service role"
  on public.users
  for all
  using (true);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
comment on table public.users is 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†ï¼ˆGoogle Sheetsã‹ã‚‰åŒæœŸï¼‰';
comment on column public.users.id is 'ãƒ¦ãƒ¼ã‚¶ãƒ¼UUIDï¼ˆæ°¸ç¶šçš„ãªè­˜åˆ¥å­ï¼‰';
comment on column public.users.email is 'Google Sheetsã¨ã®ç…§åˆã‚­ãƒ¼';
comment on column public.users.is_active is 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ï¼ˆSheetsã‹ã‚‰å‰Šé™¤æ™‚falseï¼‰';
```

**2. `db/migrations/20241215000001_create_analysis_runs_table.sql`**:
```sql
-- analysis_runsãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
create table public.analysis_runs (
    id uuid primary key default gen_random_uuid(),
    user_id uuid not null references public.users(id) on delete cascade,

    -- åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    analysis_type text not null check (analysis_type in ('domi', 'aga')),
    days_analyzed integer not null,
    source_used text not null check (source_used in ('notion', 'gdocs')),

    -- åˆ†æçµæœ
    content text,
    stats_summary text,

    -- å®Ÿè¡Œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    status text not null check (status in ('pending', 'running', 'completed', 'failed')),
    error_message text,
    trigger_type text check (trigger_type in ('github_actions', 'manual', 'cron', 'api')),
    trigger_id text,

    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at timestamptz default now(),
    completed_at timestamptz
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
create index idx_analysis_runs_user_id on public.analysis_runs(user_id);
create index idx_analysis_runs_status on public.analysis_runs(status);
create index idx_analysis_runs_created_at on public.analysis_runs(created_at desc);

-- RLSæœ‰åŠ¹åŒ–
alter table public.analysis_runs enable row level security;

-- ãƒãƒªã‚·ãƒ¼
create policy "Enable all access for service role"
  on public.analysis_runs
  for all
  using (true);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
comment on table public.analysis_runs is 'åˆ†æå®Ÿè¡Œå±¥æ­´';
comment on column public.analysis_runs.content is 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é…ä¿¡ã—ãŸãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡ï¼ˆMarkdownå½¢å¼ï¼‰';
comment on column public.analysis_runs.stats_summary is 'çµ±è¨ˆã‚µãƒãƒªãƒ¼ï¼ˆä¾‹: ç›´è¿‘7æ—¥é–“: 21ä»¶ã€å¹³å‡5092æ–‡å­—ï¼‰';
```

**3. `db/migrations/20241215000002_create_deliveries_table.sql`**:
```sql
-- deliveriesãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
create table public.deliveries (
    id uuid primary key default gen_random_uuid(),
    analysis_run_id uuid not null references public.analysis_runs(id) on delete cascade,

    -- é…ä¿¡æ–¹æ³•
    delivery_method text not null check (delivery_method in (
        'email_html', 'email_text', 'console', 'file_text', 'file_html'
    )),
    email_to text,

    -- é…ä¿¡çµæœ
    status text not null check (status in ('pending', 'sent', 'failed')),
    error_message text,

    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at timestamptz default now(),
    sent_at timestamptz
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
create index idx_deliveries_analysis_run_id on public.deliveries(analysis_run_id);
create index idx_deliveries_status on public.deliveries(status);
create index idx_deliveries_created_at on public.deliveries(created_at desc);

-- RLSæœ‰åŠ¹åŒ–
alter table public.deliveries enable row level security;

-- ãƒãƒªã‚·ãƒ¼
create policy "Enable all access for service role"
  on public.deliveries
  for all
  using (true);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
comment on table public.deliveries is 'é…ä¿¡å±¥æ­´ï¼ˆé…ä¿¡æ–¹æ³•ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰';
```

**4. `db/migrations/20241215000003_create_execution_history_view.sql`**:
```sql
-- å®Ÿè¡Œå±¥æ­´ãƒ“ãƒ¥ãƒ¼ä½œæˆ
create or replace view public.execution_history as
select
    u.email,
    u.user_name,
    ar.analysis_type,
    ar.days_analyzed,
    ar.source_used,
    ar.status as analysis_status,
    ar.trigger_type,
    ar.trigger_id,
    ar.created_at as analysis_started_at,
    ar.completed_at as analysis_completed_at,
    d.delivery_method,
    d.email_to,
    d.status as delivery_status,
    d.sent_at as delivery_sent_at,
    d.error_message as delivery_error
from public.analysis_runs ar
join public.users u on ar.user_id = u.id
left join public.deliveries d on d.analysis_run_id = ar.id
order by ar.created_at desc;

-- ã‚³ãƒ¡ãƒ³ãƒˆ
comment on view public.execution_history is 'å®Ÿè¡Œå±¥æ­´ã®çµ±åˆãƒ“ãƒ¥ãƒ¼ï¼ˆåˆ†æ + é…ä¿¡ï¼‰';
```

**ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨æ–¹æ³•**:

```bash
# Supabase CLIã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰
brew install supabase/tap/supabase

# Supabaseãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³
supabase login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ³ã‚¯
supabase link --project-ref YOUR_PROJECT_REF

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
supabase db push

# ã¾ãŸã¯ã€Supabase Dashboard > SQL Editorã§å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’é †ç•ªã«å®Ÿè¡Œ
```

---

### Step 3: Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆï¼ˆ5åˆ†ï¼‰

**`db/client.py`**:
```python
"""Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
import os
from supabase import create_client, Client
from functools import lru_cache


@lru_cache(maxsize=1)
def get_supabase_client() -> Client:
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—"""
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_KEY')

    if not url or not key:
        raise ValueError(
            "SUPABASE_URLã¨SUPABASE_KEYã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„"
        )

    return create_client(url, key)
```

---

**`models/__init__.py`**:
```python
"""ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"""
from models.user import User
from models.analysis_run import AnalysisRun
from models.delivery import Delivery

__all__ = ['User', 'AnalysisRun', 'Delivery']
```

**`models/user.py`**:
```python
"""Userãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"""
from typing import Optional, List, Dict
from datetime import datetime
from db.client import get_supabase_client
from utils.logger import logger


class User:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«

    è²¬å‹™:
    - Supabaseã¨ã®æ°¸ç¶šåŒ–ãƒ­ã‚¸ãƒƒã‚¯
    - Google Sheetsã¨ã®åŒæœŸãƒ­ã‚¸ãƒƒã‚¯
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    """

    def __init__(
        self,
        id: Optional[str] = None,
        email: str = None,
        user_name: str = None,
        notion_api_key: Optional[str] = None,
        google_docs_url: Optional[str] = None,
        language: str = 'japanese',
        is_active: bool = True,
        **kwargs
    ):
        self.id = id
        self.email = email
        self.user_name = user_name
        self.notion_api_key = notion_api_key
        self.google_docs_url = google_docs_url
        self.language = language
        self.is_active = is_active
        self.source_type = self._detect_source_type()

    def _detect_source_type(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        has_notion = bool(self.notion_api_key)
        has_gdocs = bool(self.google_docs_url)

        if has_notion and has_gdocs:
            return 'both'
        elif has_notion:
            return 'notion'
        elif has_gdocs:
            return 'gdocs'
        return 'unknown'

    @classmethod
    def find_by_email(cls, email: str) -> Optional['User']:
        """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢"""
        supabase = get_supabase_client()
        result = supabase.table('users').select('*').eq('email', email).execute()

        if result.data:
            return cls(**result.data[0])
        return None

    @classmethod
    def find_all_active(cls) -> List['User']:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã™ã¹ã¦å–å¾—"""
        supabase = get_supabase_client()
        result = supabase.table('users').select('*').eq('is_active', True).execute()

        return [cls(**row) for row in result.data]

    def save(self) -> 'User':
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä¿å­˜ï¼ˆæ–°è¦ä½œæˆã¾ãŸã¯æ›´æ–°ï¼‰"""
        supabase = get_supabase_client()

        if self.id:
            # æ›´æ–°
            result = supabase.table('users').update({
                'user_name': self.user_name,
                'language': self.language,
                'notion_api_key': self.notion_api_key,
                'google_docs_url': self.google_docs_url,
                'source_type': self.source_type,
                'is_active': self.is_active,
                'updated_at': 'now()'
            }).eq('id', self.id).execute()

            logger.info(f"âœ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ›´æ–°: {self.email}", "user")
        else:
            # æ–°è¦ä½œæˆ
            result = supabase.table('users').insert({
                'email': self.email,
                'user_name': self.user_name,
                'language': self.language,
                'notion_api_key': self.notion_api_key,
                'google_docs_url': self.google_docs_url,
                'source_type': self.source_type,
                'is_active': self.is_active
            }).execute()

            self.id = result.data[0]['id']
            logger.success(f"âœ¨ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ: {self.email} ({self.id})", "user")

        return self

    def deactivate(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–"""
        self.is_active = False
        self.save()
        logger.warning(f"âš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–: {self.email}", "user")

    def update_last_analysis_at(self):
        """æœ€çµ‚åˆ†ææ™‚åˆ»ã‚’æ›´æ–°"""
        supabase = get_supabase_client()
        supabase.table('users').update({
            'last_analysis_at': 'now()'
        }).eq('id', self.id).execute()

    @classmethod
    def sync_from_google_sheets(
        cls,
        sheets_data: List[Dict]
    ) -> List['User']:
        """Google Sheetsã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’åŒæœŸ

        Args:
            sheets_data: Google Sheetsã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿

        Returns:
            åŒæœŸã•ã‚ŒãŸUserã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        sheets_emails = [row['email_to'] for row in sheets_data]

        # æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—
        supabase = get_supabase_client()
        existing_result = supabase.table('users').select('id, email').execute()
        existing_users = {u['email']: u['id'] for u in existing_result.data}

        synced_users = []

        # Google Sheetsã®å„è¡Œã‚’å‡¦ç†
        for row in sheets_data:
            email = row['email_to']

            if email in existing_users:
                # æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼æ›´æ–°
                user = cls.find_by_email(email)
                user.user_name = row['user_name']
                user.language = row.get('language', 'japanese')
                user.notion_api_key = row.get('notion_api_key')
                user.google_docs_url = row.get('google_docs_url')
                user.is_active = True
                user.save()
            else:
                # æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
                user = cls(
                    email=email,
                    user_name=row['user_name'],
                    language=row.get('language', 'japanese'),
                    notion_api_key=row.get('notion_api_key'),
                    google_docs_url=row.get('google_docs_url'),
                    is_active=True
                )
                user.save()

            synced_users.append(user)

        # Sheetsã‹ã‚‰å‰Šé™¤ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–
        removed_emails = set(existing_users.keys()) - set(sheets_emails)
        for email in removed_emails:
            user = cls.find_by_email(email)
            if user:
                user.deactivate()

        logger.complete(f"åŒæœŸå®Œäº†: {len(synced_users)}äºº", "user")
        return synced_users

    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆå®Ÿè¡Œç”¨ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        return {
            'user_id': self.id,
            'email_to': self.email,
            'user_name': self.user_name,
            'notion_api_key': self.notion_api_key,
            'google_docs_url': self.google_docs_url,
            'language': self.language
        }
```

**`models/analysis_run.py`**:
```python
"""AnalysisRunãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"""
from typing import Optional
import os
from db.client import get_supabase_client
from utils.logger import logger


class AnalysisRun:
    """åˆ†æå®Ÿè¡Œãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«

    è²¬å‹™:
    - åˆ†æå®Ÿè¡Œå±¥æ­´ã®æ°¸ç¶šåŒ–
    - åˆ†æçµæœã®ä¿å­˜
    - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç®¡ç†
    """

    def __init__(
        self,
        user_id: str,
        analysis_type: str,
        days_analyzed: int,
        source_used: str,
        content: Optional[str] = None,
        stats_summary: Optional[str] = None,
        status: str = 'pending',
        error_message: Optional[str] = None,
        trigger_type: Optional[str] = None,
        trigger_id: Optional[str] = None,
        id: Optional[str] = None,
        **kwargs
    ):
        self.id = id
        self.user_id = user_id
        self.analysis_type = analysis_type
        self.days_analyzed = days_analyzed
        self.source_used = source_used
        self.content = content
        self.stats_summary = stats_summary
        self.status = status
        self.error_message = error_message
        self.trigger_type = trigger_type or self._detect_trigger_type()
        self.trigger_id = trigger_id or self._detect_trigger_id()

    def _detect_trigger_type(self) -> str:
        """ãƒˆãƒªã‚¬ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º"""
        if os.getenv('GITHUB_ACTIONS'):
            return 'github_actions'
        return 'manual'

    def _detect_trigger_id(self) -> Optional[str]:
        """ãƒˆãƒªã‚¬ãƒ¼IDã‚’æ¤œå‡º"""
        if os.getenv('GITHUB_ACTIONS'):
            return os.getenv('GITHUB_RUN_ID')
        return None

    @classmethod
    def create(
        cls,
        user_id: str,
        analysis_type: str,
        days_analyzed: int,
        source_used: str
    ) -> 'AnalysisRun':
        """åˆ†æå®Ÿè¡Œã‚’é–‹å§‹ï¼ˆpendingã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ä½œæˆï¼‰"""
        run = cls(
            user_id=user_id,
            analysis_type=analysis_type,
            days_analyzed=days_analyzed,
            source_used=source_used,
            status='pending'
        )
        run.save()
        return run

    def save(self) -> 'AnalysisRun':
        """åˆ†æå®Ÿè¡Œã‚’ä¿å­˜"""
        supabase = get_supabase_client()

        if self.id:
            # æ›´æ–°
            result = supabase.table('analysis_runs').update({
                'status': self.status,
                'content': self.content,
                'stats_summary': self.stats_summary,
                'error_message': self.error_message,
                'completed_at': 'now()' if self.status in ['completed', 'failed'] else None
            }).eq('id', self.id).execute()
        else:
            # æ–°è¦ä½œæˆ
            result = supabase.table('analysis_runs').insert({
                'user_id': self.user_id,
                'analysis_type': self.analysis_type,
                'days_analyzed': self.days_analyzed,
                'source_used': self.source_used,
                'content': self.content,
                'stats_summary': self.stats_summary,
                'status': self.status,
                'error_message': self.error_message,
                'trigger_type': self.trigger_type,
                'trigger_id': self.trigger_id
            }).execute()

            self.id = result.data[0]['id']
            logger.info(f"åˆ†æå®Ÿè¡Œé–‹å§‹: {self.id}", "analysis")

        return self

    def mark_running(self):
        """å®Ÿè¡Œä¸­ã«å¤‰æ›´"""
        self.status = 'running'
        self.save()

    def mark_completed(self, content: str, stats_summary: str):
        """å®Œäº†ã«å¤‰æ›´"""
        self.status = 'completed'
        self.content = content
        self.stats_summary = stats_summary
        self.save()
        logger.success(f"âœ… åˆ†æå®Œäº†: {self.id}", "analysis")

    def mark_failed(self, error_message: str):
        """å¤±æ•—ã«å¤‰æ›´"""
        self.status = 'failed'
        self.error_message = error_message
        self.save()
        logger.error(f"âŒ åˆ†æå¤±æ•—: {self.id}", "analysis", error=error_message)
```

**`models/delivery.py`**:
```python
"""Deliveryãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"""
from typing import Optional
from db.client import get_supabase_client
from utils.logger import logger


class Delivery:
    """é…ä¿¡ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«

    è²¬å‹™:
    - é…ä¿¡å±¥æ­´ã®æ°¸ç¶šåŒ–
    - é…ä¿¡ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç®¡ç†
    """

    def __init__(
        self,
        analysis_run_id: str,
        delivery_method: str,
        email_to: Optional[str] = None,
        status: str = 'pending',
        error_message: Optional[str] = None,
        id: Optional[str] = None,
        **kwargs
    ):
        self.id = id
        self.analysis_run_id = analysis_run_id
        self.delivery_method = delivery_method
        self.email_to = email_to
        self.status = status
        self.error_message = error_message

    @classmethod
    def create(
        cls,
        analysis_run_id: str,
        delivery_method: str,
        email_to: Optional[str] = None
    ) -> 'Delivery':
        """é…ä¿¡ã‚’ä½œæˆ"""
        delivery = cls(
            analysis_run_id=analysis_run_id,
            delivery_method=delivery_method,
            email_to=email_to,
            status='pending'
        )
        delivery.save()
        return delivery

    def save(self) -> 'Delivery':
        """é…ä¿¡ã‚’ä¿å­˜"""
        supabase = get_supabase_client()

        if self.id:
            # æ›´æ–°
            result = supabase.table('deliveries').update({
                'status': self.status,
                'error_message': self.error_message,
                'sent_at': 'now()' if self.status == 'sent' else None
            }).eq('id', self.id).execute()
        else:
            # æ–°è¦ä½œæˆ
            result = supabase.table('deliveries').insert({
                'analysis_run_id': self.analysis_run_id,
                'delivery_method': self.delivery_method,
                'email_to': self.email_to,
                'status': self.status
            }).execute()

            self.id = result.data[0]['id']
            logger.info(f"é…ä¿¡é–‹å§‹: {self.delivery_method}", "delivery")

        return self

    def mark_sent(self):
        """é€ä¿¡å®Œäº†ã«å¤‰æ›´"""
        self.status = 'sent'
        self.save()
        logger.success(f"âœ… é…ä¿¡å®Œäº†: {self.delivery_method}", "delivery")

    def mark_failed(self, error_message: str):
        """é€ä¿¡å¤±æ•—ã«å¤‰æ›´"""
        self.status = 'failed'
        self.error_message = error_message
        self.save()
        logger.error(f"âŒ é…ä¿¡å¤±æ•—: {self.delivery_method}", "delivery",
                    error=error_message)
```

---

### Step 5: ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆ5åˆ†ï¼‰

**ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼ˆ`.env`ï¼‰**:
```bash
# Supabaseè¨­å®š
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_KEY=sb_secret_xxxxx...

# æ—¢å­˜ã®ç’°å¢ƒå¤‰æ•°
OPENAI_API_KEY=sk-...
NOTION_API_KEY=ntn_...
# ...
```

**GitHub Actionsç”¨ï¼ˆSecretsè¨­å®šï¼‰**:

Settings > Secrets and variables > Actions > New repository secret

- `SUPABASE_URL`
- `SUPABASE_KEY`

---

### Step 6: ä¾å­˜é–¢ä¿‚è¿½åŠ ï¼ˆ5åˆ†ï¼‰

`pyproject.toml`ã«è¿½åŠ :

```toml
[project]
dependencies = [
    "notion-client>=2.3.0",
    "openai>=1.84.0",
    "python-dotenv>=1.1.0",
    "google-auth>=2.29.0",
    "google-api-python-client>=2.137.0",
    "supabase>=2.0.0",  # ğŸ†• è¿½åŠ 
]
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
```bash
uv sync
```

---

### Step 7: read_spreadsheet_and_execute.py ã®å®Ÿè£…ï¼ˆ30åˆ†ï¼‰

**ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸå®Ÿè£…**:

```python
# read_spreadsheet_and_execute.py

import argparse
import sys
import os
import subprocess
from typing import List, Dict
from utils.logger import logger
from utils.google_service import get_google_service
from models.user import User


class GoogleSheetsReader:
    """Google Sheetsã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""

    def __init__(self):
        self.sheets_service = get_google_service().get_sheets_service()

    def read_user_data(self, spreadsheet_id: str, range_name: str = "A1:E") -> List[Dict]:
        """Google Sheetsã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        result = self.sheets_service.spreadsheets().values().get(
            spreadsheetId=spreadsheet_id,
            range=range_name
        ).execute()

        rows = result.get('values', [])

        if not rows or len(rows) < 2:
            logger.warning("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "sheets")
            return []

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
        data_rows = rows[1:]

        user_data_list = []
        for row in data_rows:
            if not row or len(row) == 0:
                continue

            # åˆ—ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
            while len(row) < 5:
                row.append('')

            user_data = {
                'email_to': row[0].strip(),
                'notion_api_key': row[1].strip() if row[1] else None,
                'google_docs_url': row[2].strip() if row[2] else None,
                'user_name': row[3].strip(),
                'language': row[4].strip() if row[4] else 'japanese'
            }

            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå¿…é ˆ
            if user_data['email_to']:
                user_data_list.append(user_data)

        return user_data_list


def execute_pickles_for_user(user: User, analysis_type: str,
                             delivery_methods: str, days: int = 7) -> bool:
    """æŒ‡å®šã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦Picklesã‚’å®Ÿè¡Œ

    Args:
        user: Userãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
        analysis_type: åˆ†æã‚¿ã‚¤ãƒ—ï¼ˆdomi/agaï¼‰
        delivery_methods: é…ä¿¡æ–¹æ³•
        days: å–å¾—æ—¥æ•°

    Returns:
        æˆåŠŸã—ãŸã‹ã©ã†ã‹
    """

    logger.info(f"ğŸ¯ {user.user_name} ã®åˆ†æé–‹å§‹", "execution",
               email=user.email)

    user_data = user.to_dict()

    cmd = [
        sys.executable, "main.py",
        "--user-id", user.id,  # UUIDã‚’æ¸¡ã™
        "--analysis", analysis_type,
        "--delivery", delivery_methods,
        "--days", str(days),
        "--user-name", user_data['user_name'],
        "--email-to", user_data['email_to'],
        "--language", user_data['language']
    ]

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¿½åŠ 
    if user.notion_api_key:
        cmd.extend(["--source", "notion",
                   "--notion-api-key", user.notion_api_key])
    elif user.google_docs_url:
        cmd.extend(["--source", "gdocs",
                   "--gdocs-url", user.google_docs_url])

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)

        if result.returncode == 0:
            # æœ€çµ‚åˆ†ææ™‚åˆ»ã‚’æ›´æ–°
            user.update_last_analysis_at()
            logger.success(f"âœ… {user.user_name} å®Œäº†", "execution")
            return True
        else:
            logger.error(f"âŒ {user.user_name} å¤±æ•—", "execution",
                        error=result.stderr)
            return False

    except Exception as e:
        logger.error(f"âŒ {user.user_name} ã‚¨ãƒ©ãƒ¼", "execution",
                    error=str(e))
        return False


def main():
    parser = argparse.ArgumentParser(
        description="Pickles Multi-User Execution with Supabase Sync"
    )

    parser.add_argument("--spreadsheet-id", required=True,
                       help="Google Spreadsheetsã®ID")
    parser.add_argument("--range", default="A1:E",
                       help="èª­ã¿è¾¼ã¿ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: A1:Eï¼‰")
    parser.add_argument("--analysis", default="domi",
                       choices=["domi", "aga"], help="åˆ†æã‚¿ã‚¤ãƒ—")
    parser.add_argument("--delivery", default="email_html",
                       help="é…ä¿¡æ–¹æ³•ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šå¯ï¼‰")
    parser.add_argument("--days", type=int, default=7,
                       help="å–å¾—æ—¥æ•°")

    args = parser.parse_args()

    try:
        logger.start("Google Sheetsèª­ã¿è¾¼ã¿é–‹å§‹", "sheets",
                    spreadsheet_id=args.spreadsheet_id)

        # 1. Google Sheetsã‹ã‚‰èª­ã¿è¾¼ã¿
        sheets_reader = GoogleSheetsReader()
        sheets_data = sheets_reader.read_user_data(args.spreadsheet_id)

        logger.info(f"Google Sheetsã‹ã‚‰{len(sheets_data)}äººèª­ã¿è¾¼ã¿", "sheets")

        # 2. Userãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã§åŒæœŸï¼ˆè‡ªå‹•çš„ã«Supabaseã¨åŒæœŸï¼‰
        users = User.sync_from_google_sheets(sheets_data)

        if not users:
            logger.error("å®Ÿè¡Œå¯èƒ½ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "execution")
            sys.exit(1)

        # 3. å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦å®Ÿè¡Œ
        success_count = 0
        total_count = len(users)

        logger.info(f"ğŸ“Š {total_count}äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦åˆ†æå®Ÿè¡Œ", "execution")

        for i, user in enumerate(users, 1):
            logger.info(f"[{i}/{total_count}] {user.user_name}", "execution")

            if execute_pickles_for_user(user, args.analysis,
                                       args.delivery, args.days):
                success_count += 1

        # çµæœã‚µãƒãƒªãƒ¼
        logger.complete("å®Ÿè¡Œå®Œäº†", "execution",
                       success=success_count,
                       total=total_count,
                       failed=total_count - success_count)

        sys.exit(0 if success_count > 0 else 1)

    except Exception as e:
        logger.error("å®Ÿè¡Œã‚¨ãƒ©ãƒ¼", "execution", error=str(e))
        sys.exit(1)


if __name__ == "__main__":
    main()
```

**å¤‰æ›´ãƒã‚¤ãƒ³ãƒˆ**:
- `SupabaseUserSync`ã‚¯ãƒ©ã‚¹ã‚’å‰Šé™¤ â†’ `User.sync_from_google_sheets()` ã«çµ±åˆ
- `execute_pickles_for_user`ã¯`User`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å—ã‘å–ã‚‹
- ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ï¼ˆ`user.to_dict()`, `user.update_last_analysis_at()`ï¼‰
- ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ãŒmodels/å†…ã«é›†ç´„ã•ã‚Œã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã‚·ãƒ³ãƒ—ãƒ«ã«

---

### Step 8: main.py ã®ä¿®æ­£ï¼ˆ20åˆ†ï¼‰

**ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸå®Ÿè£…**:

```python
# main.py

import argparse
from models.analysis_run import AnalysisRun
from models.delivery import Delivery
from utils.logger import logger

# ... æ—¢å­˜ã®import


def main():
    parser = argparse.ArgumentParser(
        description="Pickles - Personal Journal Analysis System"
    )

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDå¼•æ•°è¿½åŠ 
    parser.add_argument("--user-id", help="User UUID from Supabase")

    # æ—¢å­˜ã®å¼•æ•°
    parser.add_argument("--source", choices=["notion", "gdocs"], default="notion")
    parser.add_argument("--analysis", choices=["domi", "aga"], default="domi")
    parser.add_argument("--delivery", default="console")
    parser.add_argument("--days", type=int, default=7)
    parser.add_argument("--email-to", help="Email address")
    # ... ä»–ã®å¼•æ•°

    args = parser.parse_args()

    analysis_run = None

    try:
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã§åˆ†æå®Ÿè¡Œã‚’è¨˜éŒ²é–‹å§‹
        if args.user_id:
            analysis_run = AnalysisRun.create(
                user_id=args.user_id,
                analysis_type=args.analysis,
                days_analyzed=args.days,
                source_used=args.source
            )
            analysis_run.mark_running()

        # PicklesSystemã§åˆ†æå®Ÿè¡Œ
        system = PicklesSystem(
            source_type=args.source,
            analysis_type=args.analysis,
            # ...
        )

        result = system.run_analysis()

        if result.success:
            # åˆ†æå®Œäº†ã‚’è¨˜éŒ²
            if analysis_run:
                analysis_run.mark_completed(
                    content=result.report_text,
                    stats_summary=result.stats_summary
                )

                # é…ä¿¡å±¥æ­´ã‚’è¨˜éŒ²
                delivery_methods = args.delivery.split(',')
                for method in delivery_methods:
                    delivery = Delivery.create(
                        analysis_run_id=analysis_run.id,
                        delivery_method=method.strip(),
                        email_to=args.email_to if 'email' in method else None
                    )

                    # é…ä¿¡å®Ÿè¡Œï¼ˆæ—¢å­˜ã®outputs/ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                    try:
                        # ... é…ä¿¡å‡¦ç† ...
                        delivery.mark_sent()
                    except Exception as e:
                        delivery.mark_failed(str(e))

                logger.success(f"âœ… åˆ†æå®Œäº†: {analysis_run.id}", "main")
        else:
            if analysis_run:
                analysis_run.mark_failed(result.error_message)

    except Exception as e:
        logger.error("åˆ†æã‚¨ãƒ©ãƒ¼", "main", error=str(e))
        if analysis_run:
            analysis_run.mark_failed(str(e))
        sys.exit(1)


if __name__ == "__main__":
    main()
```

**å¤‰æ›´ãƒã‚¤ãƒ³ãƒˆ**:
- `AnalysisRun.create()` ã§åˆ†æé–‹å§‹ã‚’è¨˜éŒ²
- `analysis_run.mark_running()` â†’ `mark_completed()` ã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é·ç§»
- `Delivery.create()` ã§é…ä¿¡å±¥æ­´ã‚’è¨˜éŒ²
- ã‚¨ãƒ©ãƒ¼æ™‚ã¯ `mark_failed()` ã§è¨˜éŒ²
- ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ãŒãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ‹…å½“ã—ã€main.pyã¯ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿

---

###Step 9: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆï¼ˆ10åˆ†ï¼‰

```bash
# 1. ç’°å¢ƒå¤‰æ•°ç¢ºèª
cat .env | grep SUPABASE

# 2. read_spreadsheet_and_execute.pyã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python read_spreadsheet_and_execute.py \
  --spreadsheet-id YOUR_SHEET_ID \
  --analysis domi \
  --delivery console

# 3. Supabaseã§ç¢ºèª
# Dashboard > Table Editor > users
# æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒUUIDä»˜ãã§ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

# 4. åˆ†æå®Ÿè¡Œå±¥æ­´ã‚’ç¢ºèª
# Dashboard > Table Editor > analysis_runs
# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒcompletedã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª
```

---

### Step 10: GitHub Actionsæ›´æ–°ï¼ˆ5åˆ†ï¼‰

`.github/workflows/pickles-report-production.yml`:

```yaml
name: Pickles Report Production

on:
  schedule:
    - cron: "0 22 * * 0"  # æ¯é€±æ—¥æ›œ22:00 UTC (æœˆæ›œ7:00 JST)
  workflow_dispatch:

jobs:
  analyze-users:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install uv
        run: pip install uv

      - name: Install dependencies
        run: uv sync

      - name: Run analysis for all users
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }}
          EMAIL_USER: ${{ secrets.EMAIL_USER }}
          EMAIL_PASS: ${{ secrets.EMAIL_PASS }}
          EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
          EMAIL_HOST: ${{ secrets.EMAIL_HOST }}
          EMAIL_PORT: ${{ secrets.EMAIL_PORT }}
          SPREADSHEET_ID_USERS_LIST: ${{ secrets.SPREADSHEET_ID_USERS_LIST }}
        run: |
          python read_spreadsheet_and_execute.py \
            --spreadsheet-id $SPREADSHEET_ID_USERS_LIST \
            --analysis domi \
            --delivery email_html \
            --days 7
```

---

## ğŸ¯ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Q: Google Sheetsã«è¿½åŠ ã—ãŸã®ã«åˆ†æãŒå®Ÿè¡Œã•ã‚Œãªã„

**A**: æ¬¡ã®ã„ãšã‚Œã‹ã‚’ç¢ºèª:
1. æ¬¡å›ã®GitHub Actionså®Ÿè¡Œã‚’å¾…ã¤ï¼ˆæ¯é€±æœˆæ›œ7:00 JSTï¼‰
2. æ‰‹å‹•å®Ÿè¡Œ: Actions > pickles-report-production > Run workflow
3. ãƒ­ã‚°ã‚’ç¢ºèª: âœ¨ æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™»éŒ²ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹ã‹

### Q: Notion API Keyã‚’å¤‰æ›´ã—ãŸã®ã«åæ˜ ã•ã‚Œãªã„

**A**: æ¬¡å›å®Ÿè¡Œæ™‚ã«è‡ªå‹•ã§æ›´æ–°ã•ã‚Œã¾ã™ã€‚å³åº§ã«åæ˜ ã—ãŸã„å ´åˆã¯æ‰‹å‹•å®Ÿè¡Œã€‚

### Q: èª¤ã£ã¦å‰Šé™¤ã—ã¦ã—ã¾ã£ãŸ

**A**: Google Sheetsã«å†åº¦è¿½åŠ ã™ã‚Œã°ã€æ—¢å­˜ã®UUIDã§å¾©æ´»ã—ã¾ã™ï¼ˆis_active: trueï¼‰ã€‚

### Q: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®importã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹

**A**: `models/__init__.py`ãŒæ­£ã—ãä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª:
```python
from models.user import User
from models.analysis_run import AnalysisRun
from models.delivery import Delivery

__all__ = ['User', 'AnalysisRun', 'Delivery']
```

---

## ğŸ“ˆ å°†æ¥ã®æ‹¡å¼µï¼ˆPhase 1ä»¥é™ï¼‰

### Phase 1ã§è¿½åŠ äºˆå®šã®ãƒ†ãƒ¼ãƒ–ãƒ«

```sql
-- Phase 1a: ç”Ÿã‚¸ãƒ£ãƒ¼ãƒŠãƒ«è“„ç©
journals (
    id, user_id, source_type, raw_content,
    embedding vector(1536), created_at
)

-- Phase 1b: ç™ºé…µã‚·ã‚¹ãƒ†ãƒ 
fermentation_nodes (
    id, user_id, title, fermented_content,
    layer_type, embedding vector(1536)
)

fermentation_lineage (
    parent_id, child_id, relationship_type
)
```

**äº’æ›æ€§**:
- `users.id` ã‚’å¤–éƒ¨ã‚­ãƒ¼ã¨ã—ã¦å‚ç…§å¯èƒ½
- `analysis_runs` ã¯é…ä¿¡å±¥æ­´ã¨ã—ã¦ç¶™ç¶šä½¿ç”¨
- `fermentation_nodes` ã¯ç™ºé…µåŸæ–™ã¨ã—ã¦æ–°è¦è¿½åŠ 

è©³ç´°ã¯ `FERMENTATION_DESIGN.md` ã‚’å‚ç…§ã€‚

---

## ğŸ’¡ è¨­è¨ˆåˆ¤æ–­ã®æ ¹æ‹ 

### 1. ãªãœGoogle Sheetsã‚’ç¶™ç¶šä½¿ç”¨ã™ã‚‹ã®ã‹

**ç†ç”±**:
- ä½¿ã„æ…£ã‚ŒãŸUIï¼ˆéæŠ€è¡“è€…ã§ã‚‚ç·¨é›†å¯èƒ½ï¼‰
- å³åº§ã«å¤‰æ›´å¯èƒ½ï¼ˆSupabase UIã‚ˆã‚Šæ‰‹è»½ï¼‰
- æ—¢å­˜ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç¶­æŒ

**Supabaseã®å½¹å‰²**:
- å®Ÿè¡Œå±¥æ­´ã®æ°¸ç¶šåŒ–ï¼ˆåˆ†æãƒ­ã‚°ï¼‰
- UUIDç®¡ç†ï¼ˆå°†æ¥ã®ç™ºé…µã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰

### 2. ãªãœãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ãŸã‹

**ç†ç”±**:
- **å†åˆ©ç”¨æ€§**: åŒã˜ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¤‡æ•°ç®‡æ‰€ã§ä½¿ç”¨å¯èƒ½
- **ä¿å®ˆæ€§**: ãƒ­ã‚¸ãƒƒã‚¯å¤‰æ›´æ™‚ã¯models/å†…ã®ã¿ä¿®æ­£
- **ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£**: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ç‹¬ç«‹ã—ã¦ãƒ†ã‚¹ãƒˆå¯èƒ½
- **é–¢å¿ƒã®åˆ†é›¢**: æ°¸ç¶šåŒ–ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆmodels/ï¼‰ã¨å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ï¼ˆmain.py, read_spreadsheet_and_execute.pyï¼‰ãŒåˆ†é›¢

**ä¾‹**:
```python
# âŒ ä»¥å‰ï¼ˆãƒ­ã‚¸ãƒƒã‚¯åˆ†æ•£ï¼‰
# read_spreadsheet_and_execute.pyå†…ã«SupabaseUserSyncã‚¯ãƒ©ã‚¹
# main.pyå†…ã«save_analysis_runé–¢æ•°

# âœ… ç¾åœ¨ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«é›†ç´„ï¼‰
from models.user import User
from models.analysis_run import AnalysisRun

users = User.sync_from_google_sheets(sheets_data)
analysis_run = AnalysisRun.create(...)
```

### 3. ãªãœãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ã™ã‚‹ã‹

**ç†ç”±**:
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†**: ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´å±¥æ­´ã‚’Gitã§è¿½è·¡
- **å†ç¾æ€§**: æ–°ã—ã„ç’°å¢ƒã§ã‚‚åŒã˜ã‚¹ã‚­ãƒ¼ãƒã‚’æ§‹ç¯‰å¯èƒ½
- **ãƒãƒ¼ãƒ å”æ¥­**: ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ã‚’æ˜ç¤ºçš„ã«å…±æœ‰
- **Supabase CLIé€£æº**: `supabase db push`ã§ä¸€æ‹¬é©ç”¨

---

*æœ€çµ‚æ›´æ–°: 2025-12-15*
*ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 3.0ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ« + ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆï¼‰*
